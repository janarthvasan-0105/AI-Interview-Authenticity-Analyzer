[FOLDER STRUCTURE]
ai-interview-authenticity-analyzer/
‚îú‚îÄ‚îÄ app.py                     # Main Streamlit app (the code you pasted)
‚îú‚îÄ‚îÄ final_model/               # Trained text classification model
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ   ‚îú‚îÄ‚îÄ vocab.txt / merges.txt (depending on model)
‚îÇ   ‚îú‚îÄ‚îÄ special_tokens_map.json
‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ   ‚îî‚îÄ‚îÄ ...                    # Any other HF model files
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ README.md                  # This file
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ sample_video.mp4       # Optional sample video
‚îÇ   ‚îî‚îÄ‚îÄ screenshots/           # Optional UI screenshots
‚îî‚îÄ‚îÄ .streamlit/                # (Optional) Streamlit config
    ‚îî‚îÄ‚îÄ config.toml
# üé•üó£Ô∏èüìÑ AI Interview Authenticity Analyzer

> A multimodal system that flags whether an interview response looks **human** or **AI-assisted** ‚Äì using **video**, **audio**, and **text** together.

We built this system in **24 hours** during a hack-style sprint to tackle a growing problem:  
it's getting harder to tell if someone is genuinely answering in an interview or secretly using AI to generate or feed them responses.

To tackle this, we designed a **complete multimodal authenticity detection pipeline**:
- A **video model** to track **eye gaze**, **head movement**, and **blinks**
- An **audio model** to analyze **speech patterns** and extract a **transcript**
- A **text model** to check if the **transcript matches AI-generated patterns**
- Finally, a **fusion layer** that combines all three into one **authenticity score**

---

## üöÄ What This App Does

Given an **interview video**, the app:

1. **Extracts the audio** from the video.
2. **Transcribes speech** to text.
3. **Analyzes the video feed** for:
   - Blink frequency
   - Suspicious head movements
   - Eye-gaze deviation
4. **Analyzes the audio** for:
   - Pitch variation (monotone / flat vs natural)
   - Speaking rate vs duration
5. **Analyzes the text** for:
   - Whether the response looks like it was written by an AI model
6. **Fuses all signals** into one **final verdict**:
   - `Likely Human`
   - `Uncertain`
   - `Likely AI / Suspicious`
7. Generates an **annotated video** with:
   - Facial landmarks drawn
   - On-screen warnings like `BLINK`, `HEAD MOVEMENT`, `GAZE WARNING`

All of this is wrapped in a **Streamlit UI** so you can just upload a video and click a button.

---

## üß± System Architecture (High-Level)

**Inputs:**  
üéûÔ∏è Interview video (`.mp4`, `.mov`, `.avi`)

**Parallel Pipelines:**

1. **Video Pipeline (Non-verbal behavior)**
   - Face landmarks via `face_alignment`
   - Blink detection using **Eye Aspect Ratio (EAR)**
   - Head direction changes via nose‚Äìchin angle
   - Eye-gaze drift using pupil position within the eye crop

2. **Audio Pipeline (Voice behavior)**
   - Extracts audio using `moviepy`
   - Transcribes using `speech_recognition` (Google API)
   - Uses `librosa` to:
     - Estimate pitch
     - Measure pitch variation (natural vs robotic)
     - Compare speaking rate vs audio length

3. **Text Pipeline (Linguistic behavior)**
   - Runs transcript through a **fine-tuned text classifier** (`./final_model`)
   - Returns a **probability that the text is AI-generated**

4. **Fusion Layer**
   - Combines:
     - `text_score` (0‚Äì1)
     - `speech_score` (0‚Äì1)
     - `video_score` (0‚Äì1)
   - Weighted fusion:
     ```python
     overall_score = 0.4 * text_score + 0.4 * speech_score + 0.2 * video_score
     ```
   - Verdict logic:
     - `< 0.45` ‚Üí **Likely Human**
     - `0.45‚Äì0.65` ‚Üí **Uncertain**
     - `> 0.65` ‚Üí **Likely AI / Suspicious**

---

## üß† Core Logic & Components

2Ô∏è‚É£ SpeechAnalyzer ‚Äì Voice & Rhythm

>>Defined in code as:  class SpeechAnalyzer:
Steps:

1.Uses speech_recognition:

2.Loads the extracted .wav audio file.

3.Calls Google‚Äôs speech API to get the transcript.

4.Loads audio using librosa:

5.Computes pitch using librosa.yin.

6.Measures pitch standard deviation:

-----Low variance ‚Üí voice is too flat ‚Üí add to AI suspicion.

>>Compares:

Number of words in transcript

Audio duration - If words-per-second is too low ‚Üí adds to suspicion.

Builds a speech_score between 0 and 1: Starts with 0.0

Adds: +0.4 if pitch variation is very low ,+0.3 if speech density is abnormal ,+0.3 if no text could be recognized

Returns: speech_score (0‚Äì1) ,transcript (string)

3Ô∏è‚É£ TextAnalyzer ‚Äì AI Text Detection

>>Defined in code as:
class TextAnalyzer:
    def __init__(self, model_path="./final_model"):
        ...
>>What it does:

1.Loads a Hugging Face transformer model from ./final_model:

2.AutoTokenizer

3.AutoModelForSequenceClassification

4.Wraps them in a pipeline("text-classification").

>>Inference:

Given a transcript text, it returns: label (e.g., "AI-GENERATED" or "HUMAN", depending on your model), score (confidence between 0‚Äì1)

If the label suggests AI: text_score = score

If the label suggests human: text_score = 1 - score

If no model or empty text: returns 0.5 (neutral).

4Ô∏è‚É£ fusion_layer ‚Äì Final Decision Engine

>>Defined in code as:
def fusion_layer(text_score, speech_score, video_score):
    overall_score = 0.4 * text_score + 0.4 * speech_score + 0.2 * video_score
    ...

>>Outputs: text_score ,speech_score ,video_score ,overall_score

>>verdict ‚àà { "Likely Human", "Uncertain", "Likely AI / Suspicious" }

This fusion is what makes the system multimodal instead of relying on a single weak signal.

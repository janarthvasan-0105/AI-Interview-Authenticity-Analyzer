# ğŸ¥ğŸ—£ï¸ğŸ“„ AI Interview Authenticity Analyzer

> A multimodal system that flags whether an interview response looks **human** or **AI-assisted** â€“ using **video**, **audio**, and **text** together.

We built this system in **24 hours** during a hack-style sprint to tackle a growing problem:  
it's getting harder to tell if someone is genuinely answering in an interview or secretly using AI to generate or feed them responses.

To tackle this, we designed a **complete multimodal authenticity detection pipeline**:
- A **video model** to track **eye gaze**, **head movement**, and **blinks**
- An **audio model** to analyze **speech patterns** and extract a **transcript**
- A **text model** to check if the **transcript matches AI-generated patterns**
- Finally, a **fusion layer** that combines all three into one **authenticity score**

---
[FOLDER STRUCTURE]
ai-interview-authenticity-analyzer/
â”œâ”€â”€ app.py                     # Main Streamlit app (the code you pasted)
â”œâ”€â”€ final_model/               # Trained text classification model
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ vocab.txt / merges.txt (depending on model)
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ ...                    # Any other HF model files
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ sample_video.mp4       # Optional sample video
â”‚   â””â”€â”€ screenshots/           # Optional UI screenshots
â””â”€â”€ .streamlit/                # (Optional) Streamlit config
    â””â”€â”€ config.toml

## ğŸš€ What This App Does

Given an **interview video**, the app:

1. **Extracts the audio** from the video.
2. **Transcribes speech** to text.
3. **Analyzes the video feed** for:
   - Blink frequency
   - Suspicious head movements
   - Eye-gaze deviation
4. **Analyzes the audio** for:
   - Pitch variation (monotone / flat vs natural)
   - Speaking rate vs duration
5. **Analyzes the text** for:
   - Whether the response looks like it was written by an AI model
6. **Fuses all signals** into one **final verdict**:
   - `Likely Human`
   - `Uncertain`
   - `Likely AI / Suspicious`
7. Generates an **annotated video** with:
   - Facial landmarks drawn
   - On-screen warnings like `BLINK`, `HEAD MOVEMENT`, `GAZE WARNING`

All of this is wrapped in a **Streamlit UI** so you can just upload a video and click a button.

---

## ğŸ§± System Architecture (High-Level)

**Inputs:**  
ğŸï¸ Interview video (`.mp4`, `.mov`, `.avi`)

**Parallel Pipelines:**

1. **Video Pipeline (Non-verbal behavior)**
   - Face landmarks via `face_alignment`
   - Blink detection using **Eye Aspect Ratio (EAR)**
   - Head direction changes via noseâ€“chin angle
   - Eye-gaze drift using pupil position within the eye crop

2. **Audio Pipeline (Voice behavior)**
   - Extracts audio using `moviepy`
   - Transcribes using `speech_recognition` (Google API)
   - Uses `librosa` to:
     - Estimate pitch
     - Measure pitch variation (natural vs robotic)
     - Compare speaking rate vs audio length

3. **Text Pipeline (Linguistic behavior)**
   - Runs transcript through a **fine-tuned text classifier** (`./final_model`)
   - Returns a **probability that the text is AI-generated**

4. **Fusion Layer**
   - Combines:
     - `text_score` (0â€“1)
     - `speech_score` (0â€“1)
     - `video_score` (0â€“1)
   - Weighted fusion:
     ```python
     overall_score = 0.4 * text_score + 0.4 * speech_score + 0.2 * video_score
     ```
   - Verdict logic:
     - `< 0.45` â†’ **Likely Human**
     - `0.45â€“0.65` â†’ **Uncertain**
     - `> 0.65` â†’ **Likely AI / Suspicious**

---

## ğŸ§  Core Logic & Components

2ï¸âƒ£ SpeechAnalyzer â€“ Voice & Rhythm

>>Defined in code as:  class SpeechAnalyzer:
Steps:

1.Uses speech_recognition:

2.Loads the extracted .wav audio file.

3.Calls Googleâ€™s speech API to get the transcript.

4.Loads audio using librosa:

5.Computes pitch using librosa.yin.

6.Measures pitch standard deviation:

-----Low variance â†’ voice is too flat â†’ add to AI suspicion.

>>Compares:

Number of words in transcript

Audio duration - If words-per-second is too low â†’ adds to suspicion.

Builds a speech_score between 0 and 1: Starts with 0.0

Adds: +0.4 if pitch variation is very low ,+0.3 if speech density is abnormal ,+0.3 if no text could be recognized

Returns: speech_score (0â€“1) ,transcript (string)

3ï¸âƒ£ TextAnalyzer â€“ AI Text Detection

>>Defined in code as:
class TextAnalyzer:
    def __init__(self, model_path="./final_model"):
        ...
>>What it does:

1.Loads a Hugging Face transformer model from ./final_model:

2.AutoTokenizer

3.AutoModelForSequenceClassification

4.Wraps them in a pipeline("text-classification").

>>Inference:

Given a transcript text, it returns: label (e.g., "AI-GENERATED" or "HUMAN", depending on your model), score (confidence between 0â€“1)

If the label suggests AI: text_score = score

If the label suggests human: text_score = 1 - score

If no model or empty text: returns 0.5 (neutral).

4ï¸âƒ£ fusion_layer â€“ Final Decision Engine

>>Defined in code as:
def fusion_layer(text_score, speech_score, video_score):
    overall_score = 0.4 * text_score + 0.4 * speech_score + 0.2 * video_score
    ...

>>Outputs: text_score ,speech_score ,video_score ,overall_score

>>verdict âˆˆ { "Likely Human", "Uncertain", "Likely AI / Suspicious" }

This fusion is what makes the system multimodal instead of relying on a single weak signal.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22d6687",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_name_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 225\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m report.items():\n\u001b[32m    222\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_name_\u001b[49m == \u001b[33m\"\u001b[39m\u001b[33m_main_\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    226\u001b[39m     main()\n",
      "\u001b[31mNameError\u001b[39m: name '_name_' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_alignment\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "from scipy.spatial import distance\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Video Analyzer\n",
    "# ==================================\n",
    "class VideoAnalyzer:\n",
    "    def _init_(self):\n",
    "        self.total_blinks = 0\n",
    "        self.prev_ear = 0\n",
    "        self.blink_thresh = 0.2275\n",
    "        self.prev_head_dir = 0\n",
    "        self.head_turn_thresh = 0.25\n",
    "        self.sus_head_movements = 0\n",
    "        self.gaze_warnings = 0\n",
    "        self.gaze_history = deque(maxlen=10)\n",
    "        self.center_x = 0.5\n",
    "        self.calibrated = False\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def eye_aspect_ratio(self, eye):\n",
    "        A = distance.euclidean(eye[1], eye[5])\n",
    "        B = distance.euclidean(eye[2], eye[4])\n",
    "        C = distance.euclidean(eye[0], eye[3])\n",
    "        return (A + B) / (2.0 * C)\n",
    "\n",
    "    def detect_pupil(self, eye_img):\n",
    "        if eye_img.size == 0:\n",
    "            return None\n",
    "        gray = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.equalizeHist(gray)\n",
    "        _, thresh = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY_INV)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) == 0:\n",
    "            return None\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        return (x + w // 2, y + h // 2)\n",
    "\n",
    "    def crop_eye(self, frame, eye_points):\n",
    "        x_min, x_max = int(np.min(eye_points[:, 0])), int(np.max(eye_points[:, 0]))\n",
    "        y_min, y_max = int(np.min(eye_points[:, 1])), int(np.max(eye_points[:, 1]))\n",
    "        return frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    def get_head_direction(self, landmarks):\n",
    "        nose, chin = landmarks[30], landmarks[8]\n",
    "        return np.arctan2(chin[1] - nose[1], chin[0] - nose[0])\n",
    "\n",
    "    def calibrate_center(self, frame, landmarks):\n",
    "        left_eye, right_eye = landmarks[36:42], landmarks[42:48]\n",
    "        eyes = [left_eye, right_eye]\n",
    "        x_vals = []\n",
    "        for eye_points in eyes:\n",
    "            eye_img = self.crop_eye(frame, eye_points)\n",
    "            pupil = self.detect_pupil(eye_img)\n",
    "            if pupil:\n",
    "                norm_x = pupil[0] / (eye_img.shape[1] + 1e-6)\n",
    "                x_vals.append(norm_x)\n",
    "        if x_vals:\n",
    "            self.center_x = np.mean(x_vals)\n",
    "            self.calibrated = True\n",
    "\n",
    "    def process_landmarks(self, frame, landmarks):\n",
    "        left_eye, right_eye = landmarks[36:42], landmarks[42:48]\n",
    "\n",
    "        # Blink detection\n",
    "        ear = (self.eye_aspect_ratio(left_eye) + self.eye_aspect_ratio(right_eye)) / 2.0\n",
    "        if ear < self.blink_thresh and self.prev_ear >= self.blink_thresh:\n",
    "            self.total_blinks += 1\n",
    "        self.prev_ear = ear\n",
    "\n",
    "        # Head movement\n",
    "        head_dir = self.get_head_direction(landmarks)\n",
    "        if abs(head_dir - self.prev_head_dir) > self.head_turn_thresh:\n",
    "            self.sus_head_movements += 1\n",
    "        self.prev_head_dir = head_dir\n",
    "\n",
    "        # Eye gaze\n",
    "        for eye_points in [left_eye, right_eye]:\n",
    "            eye_img = self.crop_eye(frame, eye_points)\n",
    "            pupil = self.detect_pupil(eye_img)\n",
    "            if pupil:\n",
    "                norm_x = pupil[0] / (eye_img.shape[1] + 1e-6)\n",
    "                self.gaze_history.append(norm_x)\n",
    "\n",
    "        if len(self.gaze_history) > 0:\n",
    "            avg_x = np.mean(self.gaze_history)\n",
    "            if avg_x < self.center_x - 0.1 or avg_x > self.center_x + 0.06:\n",
    "                self.gaze_warnings += 1\n",
    "\n",
    "    def final_report(self):\n",
    "        prob_ai = (\n",
    "            (self.gaze_warnings * 0.4) +\n",
    "            (self.sus_head_movements * 0.3) +\n",
    "            (self.total_blinks * 0.3)\n",
    "        )\n",
    "        prob_ai = min(prob_ai / 100.0, 1.0)\n",
    "        return round(prob_ai, 2)\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Speech Analyzer\n",
    "# ==================================\n",
    "class SpeechAnalyzer:\n",
    "    def _init_(self):\n",
    "        self.score = 0.0\n",
    "\n",
    "    def analyze_audio(self, duration=5):\n",
    "        r = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"üéô Speak now...\")\n",
    "            audio = r.record(source, duration=duration)\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print(\"Transcript:\", text)\n",
    "            y, sr_rate = librosa.load(sr.AudioData.get_wav_data(audio), sr=None)\n",
    "            pitch = librosa.yin(y, fmin=50, fmax=300)\n",
    "            pitch_std = np.std(pitch)\n",
    "            if pitch_std < 5: self.score += 0.4\n",
    "            if len(text.split()) / duration < 1.5: self.score += 0.3\n",
    "            if len(text) == 0: self.score += 0.3\n",
    "        except:\n",
    "            self.score = 0.5\n",
    "        return round(min(self.score, 1.0), 2)\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Text Analyzer\n",
    "# ==================================\n",
    "class TextAnalyzer:\n",
    "    def _init_(self, model_path=\"./results_improved/final_model\"):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.detector = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    def analyze(self, text):\n",
    "        result = self.detector(text)[0]\n",
    "        return round(result['score'], 2)\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Fusion Layer\n",
    "# ==================================\n",
    "def fusion_layer(text_score, speech_score, video_score):\n",
    "    overall_score = 0.4*text_score + 0.4*speech_score + 0.2*video_score\n",
    "    if overall_score < 0.45:\n",
    "        label, color = \"Likely Human\", \"Green\"\n",
    "    elif overall_score < 0.65:\n",
    "        label, color = \"Uncertain\", \"Yellow\"\n",
    "    else:\n",
    "        label, color = \"Likely AI / Suspicious\", \"Red\"\n",
    "    return {\n",
    "        \"text_score\": text_score,\n",
    "        \"speech_score\": speech_score,\n",
    "        \"video_score\": video_score,\n",
    "        \"overall_score\": round(overall_score, 2),\n",
    "        \"verdict\": label,\n",
    "        \"color\": color\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Main\n",
    "# ==================================\n",
    "def main():\n",
    "    # Init analyzers\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False)\n",
    "    video_analyzer = VideoAnalyzer()\n",
    "    speech_analyzer = SpeechAnalyzer()\n",
    "    text_analyzer = TextAnalyzer()\n",
    "\n",
    "    # --- Calibration (video) ---\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"Calibrating gaze... look at CENTER\")\n",
    "    start_calib = time.time()\n",
    "    while time.time() - start_calib < 3:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if preds is not None:\n",
    "            video_analyzer.calibrate_center(frame, preds[0])\n",
    "        cv2.imshow(\"Calibration\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "    print(\"Calibration done ‚úÖ\")\n",
    "\n",
    "    # --- Video collection ---\n",
    "    start_video = time.time()\n",
    "    while time.time() - start_video < 5:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if preds is not None:\n",
    "            video_analyzer.process_landmarks(frame, preds[0])\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    video_score = video_analyzer.final_report()\n",
    "\n",
    "    # --- Speech ---\n",
    "    speech_score = speech_analyzer.analyze_audio(duration=5)\n",
    "\n",
    "    # --- Text ---\n",
    "    try:\n",
    "        with open(\"transcription.txt\") as f:\n",
    "            text = f.read().strip()\n",
    "    except:\n",
    "        text = \"Sample placeholder text\"\n",
    "    text_score = text_analyzer.analyze(text)\n",
    "\n",
    "    # --- Fusion ---\n",
    "    report = fusion_layer(text_score, speech_score, video_score)\n",
    "    print(\"\\nüî• FINAL REPORT üî•\")\n",
    "    for k, v in report.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187ca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• Recording interview...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• FINAL REPORT üî•\n",
      "text_score: 1.0\n",
      "speech_score: 0.3\n",
      "video_score: 0.0\n",
      "overall_score: 0.52\n",
      "verdict: Uncertain\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_alignment\n",
    "import numpy as np\n",
    "import time\n",
    "import wave\n",
    "import pyaudio\n",
    "import librosa\n",
    "import speech_recognition as sr\n",
    "from scipy.spatial import distance\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Video Analyzer\n",
    "# ==================================\n",
    "class VideoAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.total_blinks = 0\n",
    "        self.prev_ear = 0\n",
    "        self.blink_thresh = 0.2275\n",
    "        self.prev_head_dir = 0\n",
    "        self.head_turn_thresh = 0.25\n",
    "        self.sus_head_movements = 0\n",
    "        self.gaze_warnings = 0\n",
    "        self.gaze_history = deque(maxlen=10)\n",
    "        self.center_x = 0.5\n",
    "        self.calibrated = False\n",
    "\n",
    "    def eye_aspect_ratio(self, eye):\n",
    "        A = distance.euclidean(eye[1], eye[5])\n",
    "        B = distance.euclidean(eye[2], eye[4])\n",
    "        C = distance.euclidean(eye[0], eye[3])\n",
    "        return (A + B) / (2.0 * C)\n",
    "\n",
    "    def detect_pupil(self, eye_img):\n",
    "        if eye_img.size == 0:\n",
    "            return None\n",
    "        gray = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.equalizeHist(gray)\n",
    "        _, thresh = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY_INV)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) == 0:\n",
    "            return None\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        return (x + w // 2, y + h // 2)\n",
    "\n",
    "    def crop_eye(self, frame, eye_points):\n",
    "        x_min, x_max = int(np.min(eye_points[:, 0])), int(np.max(eye_points[:, 0]))\n",
    "        y_min, y_max = int(np.min(eye_points[:, 1])), int(np.max(eye_points[:, 1]))\n",
    "        return frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    def get_head_direction(self, landmarks):\n",
    "        nose, chin = landmarks[30], landmarks[8]\n",
    "        return np.arctan2(chin[1] - nose[1], chin[0] - nose[0])\n",
    "\n",
    "    def calibrate_center(self, frame, landmarks):\n",
    "        left_eye, right_eye = landmarks[36:42], landmarks[42:48]\n",
    "        eyes = [left_eye, right_eye]\n",
    "        x_vals = []\n",
    "        for eye_points in eyes:\n",
    "            eye_img = self.crop_eye(frame, eye_points)\n",
    "            pupil = self.detect_pupil(eye_img)\n",
    "            if pupil:\n",
    "                norm_x = pupil[0] / (eye_img.shape[1] + 1e-6)\n",
    "                x_vals.append(norm_x)\n",
    "        if x_vals:\n",
    "            self.center_x = np.mean(x_vals)\n",
    "            self.calibrated = True\n",
    "\n",
    "    def process_landmarks(self, frame, landmarks):\n",
    "        left_eye, right_eye = landmarks[36:42], landmarks[42:48]\n",
    "\n",
    "        # Blink detection\n",
    "        ear = (self.eye_aspect_ratio(left_eye) + self.eye_aspect_ratio(right_eye)) / 2.0\n",
    "        if ear < self.blink_thresh and self.prev_ear >= self.blink_thresh:\n",
    "            self.total_blinks += 1\n",
    "        self.prev_ear = ear\n",
    "\n",
    "        # Head movement\n",
    "        head_dir = self.get_head_direction(landmarks)\n",
    "        if abs(head_dir - self.prev_head_dir) > self.head_turn_thresh:\n",
    "            self.sus_head_movements += 1\n",
    "        self.prev_head_dir = head_dir\n",
    "\n",
    "        # Eye gaze\n",
    "        for eye_points in [left_eye, right_eye]:\n",
    "            eye_img = self.crop_eye(frame, eye_points)\n",
    "            pupil = self.detect_pupil(eye_img)\n",
    "            if pupil:\n",
    "                norm_x = pupil[0] / (eye_img.shape[1] + 1e-6)\n",
    "                self.gaze_history.append(norm_x)\n",
    "\n",
    "        if len(self.gaze_history) > 0:\n",
    "            avg_x = np.mean(self.gaze_history)\n",
    "            if avg_x < self.center_x - 0.1 or avg_x > self.center_x + 0.06:\n",
    "                self.gaze_warnings += 1\n",
    "\n",
    "    def final_report(self):\n",
    "        prob_ai = (\n",
    "            (self.gaze_warnings * 0.4) +\n",
    "            (self.sus_head_movements * 0.3) +\n",
    "            (self.total_blinks * 0.3)\n",
    "        )\n",
    "        prob_ai = min(prob_ai / 100.0, 1.0)\n",
    "        return round(prob_ai, 2)\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Audio Recorder\n",
    "# ==================================\n",
    "def record_audio(filename=\"output_audio.wav\"):\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    CHUNK = 1024\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"üé• Interview recording... Press 'q' to stop video window.\")\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "        # Stop when video window is closed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    print(\"Audio recording stopped.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Speech Analyzer\n",
    "# ==================================\n",
    "class SpeechAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.score = 0.0\n",
    "\n",
    "    def analyze_audio(self, filename=\"output_audio.wav\"):\n",
    "        try:\n",
    "            # Transcribe\n",
    "            r = sr.Recognizer()\n",
    "            with sr.AudioFile(filename) as source:\n",
    "                audio = r.record(source)\n",
    "            text = r.recognize_google(audio)\n",
    "            print(\"Transcript:\", text)\n",
    "\n",
    "            # Pitch analysis\n",
    "            y, sr_rate = librosa.load(filename, sr=None)\n",
    "            pitch = librosa.yin(y, fmin=50, fmax=300)\n",
    "            pitch_std = np.std(pitch)\n",
    "            if pitch_std < 5: self.score += 0.4\n",
    "            if len(text.split()) / (len(y) / sr_rate) < 1.5: self.score += 0.3\n",
    "            if len(text) == 0: self.score += 0.3\n",
    "\n",
    "            return round(min(self.score, 1.0), 2), text\n",
    "        except:\n",
    "            return 0.5, \"\"\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Text Analyzer\n",
    "# ==================================\n",
    "class TextAnalyzer:\n",
    "    def __init__(self, model_path=\"./final_model\"):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.detector = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    def analyze(self, text):\n",
    "        if not text.strip():\n",
    "            return 0.5\n",
    "        result = self.detector(text)[0]\n",
    "        return round(result['score'], 2)\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Fusion Layer\n",
    "# ==================================\n",
    "def fusion_layer(text_score, speech_score, video_score):\n",
    "    overall_score = 0.4*text_score + 0.4*speech_score + 0.2*video_score\n",
    "    if overall_score < 0.45:\n",
    "        label = \"Likely Human\"\n",
    "    elif overall_score < 0.65:\n",
    "        label = \"Uncertain\"\n",
    "    else:\n",
    "        label = \"Likely AI / Suspicious\"\n",
    "    return {\n",
    "        \"text_score\": text_score,\n",
    "        \"speech_score\": speech_score,\n",
    "        \"video_score\": video_score,\n",
    "        \"overall_score\": round(overall_score, 2),\n",
    "        \"verdict\": label\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Main Pipeline\n",
    "# ==================================\n",
    "def main():\n",
    "    # Init\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False)\n",
    "    video_analyzer = VideoAnalyzer()\n",
    "\n",
    "    # Open video & audio\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Calibration\n",
    "    print(\"Calibrating gaze... look at CENTER\")\n",
    "    while not video_analyzer.calibrated:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if preds is not None:\n",
    "            video_analyzer.calibrate_center(frame, preds[0])\n",
    "        cv2.imshow(\"Calibration\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "    print(\"Calibration done ‚úÖ\")\n",
    "\n",
    "    # Start recording audio in parallel\n",
    "    record_audio(\"output_audio.wav\")\n",
    "\n",
    "    # Process video until stopped\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if preds is not None:\n",
    "            video_analyzer.process_landmarks(frame, preds[0])\n",
    "        cv2.imshow(\"Interview\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    video_score = video_analyzer.final_report()\n",
    "\n",
    "    # Speech\n",
    "    speech_analyzer = SpeechAnalyzer()\n",
    "    speech_score, transcript = speech_analyzer.analyze_audio(\"output_audio.wav\")\n",
    "\n",
    "    # Text\n",
    "    text_analyzer = TextAnalyzer()\n",
    "    text_score = text_analyzer.analyze(transcript)\n",
    "\n",
    "    # Fusion\n",
    "    report = fusion_layer(text_score, speech_score, video_score)\n",
    "    print(\"\\nüî• FINAL REPORT üî•\")\n",
    "    for k, v in report.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import face_alignment\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    \"\"\"Compute EAR for blink detection.\"\"\"\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "def crop_eye(frame, eye_points):\n",
    "    \"\"\"Crop the eye region using bounding rect.\"\"\"\n",
    "    x_min = int(np.min(eye_points[:, 0]))\n",
    "    x_max = int(np.max(eye_points[:, 0]))\n",
    "    y_min = int(np.min(eye_points[:, 1]))\n",
    "    y_max = int(np.max(eye_points[:, 1]))\n",
    "    return frame[y_min:y_max, x_min:x_max], (x_min, y_min, x_max, y_max)\n",
    "\n",
    "def detect_pupil(eye_img):\n",
    "    \"\"\"Detect pupil center using threshold + contours.\"\"\"\n",
    "    if eye_img.size == 0:\n",
    "        return None\n",
    "    gray = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "    _, thresh = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "\n",
    "    # Largest contour = pupil\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    cx, cy = x + w // 2, y + h // 2\n",
    "    return (cx, cy)\n",
    "\n",
    "def get_head_direction(landmarks):\n",
    "    \"\"\"Rough head roll detection (nose-chin slope).\"\"\"\n",
    "    nose = landmarks[30]\n",
    "    chin = landmarks[8]\n",
    "    slope = np.arctan2(chin[1] - nose[1], chin[0] - nose[0])\n",
    "    return slope\n",
    "\n",
    "# -------------------------------\n",
    "# Analyzer Class\n",
    "# -------------------------------\n",
    "\n",
    "class VideoAnalyzer:\n",
    "    def _init_(self):\n",
    "        self.total_blinks = 0\n",
    "        self.prev_ear = 0\n",
    "        self.blink_thresh = 0.2275\n",
    "\n",
    "        self.prev_head_dir = 0\n",
    "        self.head_turn_thresh = 0.25\n",
    "        self.sus_head_movements = 0\n",
    "\n",
    "        self.gaze_history = deque(maxlen=10)\n",
    "        self.gaze_counts = {\"LEFT\": 0, \"RIGHT\": 0, \"CENTER\": 0}\n",
    "        self.gaze_warnings = 0\n",
    "\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        # Calibration\n",
    "        self.center_x = 0.5  # default normalized center\n",
    "        self.calibrated = False\n",
    "\n",
    "    def calibrate_center(self, frame, landmarks):\n",
    "        \"\"\"Calibrate gaze by asking user to look at the center.\"\"\"\n",
    "        left_eye = landmarks[36:42]\n",
    "        right_eye = landmarks[42:48]\n",
    "\n",
    "        eyes = [left_eye, right_eye]\n",
    "        x_vals = []\n",
    "\n",
    "        for eye_points in eyes:\n",
    "            eye_img, _ = crop_eye(frame, eye_points)\n",
    "            pupil = detect_pupil(eye_img)\n",
    "            if pupil is not None:\n",
    "                cx, _ = pupil\n",
    "                norm_x = cx / (eye_img.shape[1] + 1e-6)\n",
    "                x_vals.append(norm_x)\n",
    "\n",
    "        if x_vals:\n",
    "            self.center_x = np.mean(x_vals)\n",
    "            self.calibrated = True\n",
    "\n",
    "    def process_landmarks(self, frame, landmarks):\n",
    "        left_eye = landmarks[36:42]\n",
    "        right_eye = landmarks[42:48]\n",
    "\n",
    "        # Blink detection\n",
    "        ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0\n",
    "        if ear < self.blink_thresh and self.prev_ear >= self.blink_thresh:\n",
    "            self.total_blinks += 1\n",
    "        self.prev_ear = ear\n",
    "\n",
    "        # Head movement\n",
    "        head_dir = get_head_direction(landmarks)\n",
    "        if abs(head_dir - self.prev_head_dir) > self.head_turn_thresh:\n",
    "            self.sus_head_movements += 1\n",
    "        self.prev_head_dir = head_dir\n",
    "\n",
    "        # Eye gaze detection\n",
    "        gaze_direction = \"CENTER\"\n",
    "        for eye_points in [left_eye, right_eye]:\n",
    "            eye_img, _ = crop_eye(frame, eye_points)\n",
    "            pupil = detect_pupil(eye_img)\n",
    "            if pupil is not None:\n",
    "                cx, _ = pupil\n",
    "                norm_x = cx / (eye_img.shape[1] + 1e-6)\n",
    "                self.gaze_history.append(norm_x)\n",
    "\n",
    "        if len(self.gaze_history) > 0:\n",
    "            avg_x = np.mean(self.gaze_history)\n",
    "\n",
    "            if avg_x < self.center_x - 0.1:\n",
    "                gaze_direction = \"RIGHT\"\n",
    "            elif avg_x > self.center_x + 0.06:\n",
    "                gaze_direction = \"LEFT\"\n",
    "            else:\n",
    "                gaze_direction = \"CENTER\"\n",
    "\n",
    "            self.gaze_counts[gaze_direction] += 1\n",
    "\n",
    "        # Warning only for left/right\n",
    "        warning = None\n",
    "        if gaze_direction in [\"LEFT\", \"RIGHT\"]:\n",
    "            warning = f\"‚ö† GAZE WARNING: {gaze_direction}\"\n",
    "            self.gaze_warnings += 1\n",
    "\n",
    "        return warning\n",
    "\n",
    "    def final_report(self):\n",
    "        elapsed = round(time.time() - self.start_time, 2)\n",
    "        # Simple AI-use probability scoring\n",
    "        prob_ai = (\n",
    "            (self.gaze_warnings * 0.4) +\n",
    "            (self.sus_head_movements * 0.3) +\n",
    "            (self.total_blinks * 0.3)\n",
    "        )\n",
    "        prob_ai = min(prob_ai / 100.0, 1.0)  # Normalize 0‚Äì1\n",
    "\n",
    "        return {\n",
    "            \"Total Time (s)\": elapsed,\n",
    "            \"Total Blinks\": self.total_blinks,\n",
    "            \"Suspected Head Movements\": self.sus_head_movements,\n",
    "            \"AI-Use Probability\": round(prob_ai, 2)\n",
    "        }\n",
    "\n",
    "# -------------------------------\n",
    "# Main Pipeline\n",
    "# -------------------------------\n",
    "\n",
    "def main():\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False)\n",
    "    analyzer = VideoAnalyzer()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"‚úÖ Video monitoring started. Look at the CENTER for calibration...\")\n",
    "\n",
    "    # Calibration phase\n",
    "    start_calib = time.time()\n",
    "    while time.time() - start_calib < 3:  # 3 seconds calibration\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if preds is not None:\n",
    "            analyzer.calibrate_center(frame, preds[0])\n",
    "        cv2.putText(frame, \"Look at CENTER for calibration\", (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        cv2.imshow(\"Video Analysis\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    print(\"‚úÖ Calibration complete.\")\n",
    "\n",
    "    # Main monitoring\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if preds is not None:\n",
    "            landmarks = preds[0]\n",
    "            warning = analyzer.process_landmarks(frame, landmarks)\n",
    "\n",
    "            if warning:\n",
    "                cv2.putText(frame, warning, (30, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Video Analysis\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Final report\n",
    "    final_report = analyzer.final_report()\n",
    "    print(\"\\n‚úÖ Final Report:\")\n",
    "    for key, val in final_report.items():\n",
    "        print(f\"{key}: {val}\")\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8edcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating gaze... look at the CENTER for 3 seconds.\n",
      "‚úÖ Calibration complete.\n",
      "üé• Interview recording... Press 'q' in the video window to stop.\n",
      "üé§ Audio recording started...\n",
      "üé§ Audio recording stopped. Saving file...\n",
      "üé§ Audio saved to output_audio.wav\n",
      "Video and Audio capture complete. Analyzing results...\n",
      "üìù Transcript: romantic scenes hello good morning we will be talking about so today\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "üìπ VIDEO ANALYSIS REPORT üìπ\n",
      "=========================\n",
      "Total Time (s): 23.4\n",
      "Total Blinks: 3\n",
      "Suspected Head Movements: 3\n",
      "AI-Use Probability: 0.08\n",
      "\n",
      "=========================\n",
      "üî• FINAL FUSION REPORT üî•\n",
      "=========================\n",
      "Text Score: 1.0\n",
      "Speech Score: 0.3\n",
      "Video Score: 0.08\n",
      "Overall Score: 0.54\n",
      "Verdict: Uncertain\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_alignment\n",
    "import numpy as np\n",
    "import threading\n",
    "import wave\n",
    "import pyaudio\n",
    "import time\n",
    "import librosa\n",
    "import speech_recognition as sr\n",
    "from scipy.spatial import distance\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# ==================================\n",
    "# Video Analyzer (UPDATED based on your new code)\n",
    "# ==================================\n",
    "class VideoAnalyzer:\n",
    "    def __init__(self):\n",
    "        # General\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        # Blink Detection\n",
    "        self.total_blinks = 0\n",
    "        self.prev_ear = 0\n",
    "        self.blink_thresh = 0.2275\n",
    "\n",
    "        # Head Movement\n",
    "        self.prev_head_dir = 0\n",
    "        self.head_turn_thresh = 0.25\n",
    "        self.sus_head_movements = 0\n",
    "\n",
    "        # Gaze Tracking\n",
    "        self.gaze_history = deque(maxlen=10)\n",
    "        self.gaze_counts = {\"LEFT\": 0, \"RIGHT\": 0, \"CENTER\": 0}\n",
    "        self.gaze_warnings = 0\n",
    "\n",
    "        # Calibration\n",
    "        self.center_x = 0.5  # default normalized center\n",
    "        self.calibrated = False\n",
    "\n",
    "    def eye_aspect_ratio(self, eye):\n",
    "        \"\"\"Compute EAR for blink detection.\"\"\"\n",
    "        A = distance.euclidean(eye[1], eye[5])\n",
    "        B = distance.euclidean(eye[2], eye[4])\n",
    "        C = distance.euclidean(eye[0], eye[3])\n",
    "        return (A + B) / (2.0 * C)\n",
    "\n",
    "    def crop_eye(self, frame, eye_points):\n",
    "        \"\"\"Crop the eye region using bounding rect.\"\"\"\n",
    "        x_min = int(np.min(eye_points[:, 0]))\n",
    "        x_max = int(np.max(eye_points[:, 0]))\n",
    "        y_min = int(np.min(eye_points[:, 1]))\n",
    "        y_max = int(np.max(eye_points[:, 1]))\n",
    "        return frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    def detect_pupil(self, eye_img):\n",
    "        \"\"\"Detect pupil center using threshold + contours.\"\"\"\n",
    "        if eye_img.size == 0:\n",
    "            return None\n",
    "        gray = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.equalizeHist(gray)\n",
    "        _, thresh = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY_INV)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if len(contours) == 0:\n",
    "            return None\n",
    "\n",
    "        # Largest contour = pupil\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        return (x + w // 2, y + h // 2)\n",
    "\n",
    "    def get_head_direction(self, landmarks):\n",
    "        \"\"\"Rough head roll detection (nose-chin slope).\"\"\"\n",
    "        nose, chin = landmarks[30], landmarks[8]\n",
    "        return np.arctan2(chin[1] - nose[1], chin[0] - nose[0])\n",
    "\n",
    "    def calibrate_center(self, frame, landmarks):\n",
    "        \"\"\"Calibrate gaze by asking user to look at the center.\"\"\"\n",
    "        left_eye, right_eye = landmarks[36:42], landmarks[42:48]\n",
    "        x_vals = []\n",
    "        for eye_points in [left_eye, right_eye]:\n",
    "            eye_img = self.crop_eye(frame, eye_points)\n",
    "            pupil = self.detect_pupil(eye_img)\n",
    "            if pupil is not None and eye_img.shape[1] > 0:\n",
    "                cx, _ = pupil\n",
    "                norm_x = cx / (eye_img.shape[1] + 1e-6)\n",
    "                x_vals.append(norm_x)\n",
    "        if x_vals:\n",
    "            self.center_x = np.mean(x_vals)\n",
    "            self.calibrated = True\n",
    "\n",
    "    def process_landmarks(self, frame, landmarks):\n",
    "        \"\"\"Process landmarks to detect blinks, head movement, and eye gaze.\"\"\"\n",
    "        left_eye, right_eye = landmarks[36:42], landmarks[42:48]\n",
    "\n",
    "        # Blink detection\n",
    "        ear = (self.eye_aspect_ratio(left_eye) + self.eye_aspect_ratio(right_eye)) / 2.0\n",
    "        if ear < self.blink_thresh and self.prev_ear >= self.blink_thresh:\n",
    "            self.total_blinks += 1\n",
    "        self.prev_ear = ear\n",
    "\n",
    "        # Head movement\n",
    "        head_dir = self.get_head_direction(landmarks)\n",
    "        if self.prev_head_dir != 0 and abs(head_dir - self.prev_head_dir) > self.head_turn_thresh:\n",
    "            self.sus_head_movements += 1\n",
    "        self.prev_head_dir = head_dir\n",
    "\n",
    "        # Eye gaze detection\n",
    "        gaze_direction = \"CENTER\"\n",
    "        for eye_points in [left_eye, right_eye]:\n",
    "            eye_img = self.crop_eye(frame, eye_points)\n",
    "            pupil = self.detect_pupil(eye_img)\n",
    "            if pupil is not None and eye_img.shape[1] > 0:\n",
    "                cx, _ = pupil\n",
    "                norm_x = cx / (eye_img.shape[1] + 1e-6)\n",
    "                self.gaze_history.append(norm_x)\n",
    "\n",
    "        if len(self.gaze_history) > 0:\n",
    "            avg_x = np.mean(self.gaze_history)\n",
    "            if avg_x < self.center_x - 0.1:\n",
    "                gaze_direction = \"RIGHT\"  # User looks right, pupils move to left of eye frame\n",
    "            elif avg_x > self.center_x + 0.06:\n",
    "                gaze_direction = \"LEFT\" # User looks left, pupils move to right of eye frame\n",
    "            else:\n",
    "                gaze_direction = \"CENTER\"\n",
    "            self.gaze_counts[gaze_direction] += 1\n",
    "\n",
    "        # Return a warning for immediate feedback, only for off-center gaze\n",
    "        if gaze_direction in [\"LEFT\", \"RIGHT\"]:\n",
    "            self.gaze_warnings += 1\n",
    "            return f\"‚ö† GAZE WARNING: {gaze_direction}\"\n",
    "        return None\n",
    "\n",
    "    def final_report(self):\n",
    "        \"\"\"Generate a final dictionary with all video analysis stats.\"\"\"\n",
    "        elapsed = round(time.time() - self.start_time, 2)\n",
    "        # Simple AI-use probability scoring\n",
    "        prob_ai = (\n",
    "            (self.gaze_warnings * 0.4) +\n",
    "            (self.sus_head_movements * 0.3) +\n",
    "            (self.total_blinks * 0.3)\n",
    "        )\n",
    "        prob_ai = min(prob_ai / 100.0, 1.0)  # Normalize 0‚Äì1\n",
    "\n",
    "        return {\n",
    "            \"Total Time (s)\": elapsed,\n",
    "            \"Total Blinks\": self.total_blinks,\n",
    "            \"Suspected Head Movements\": self.sus_head_movements,\n",
    "            \"AI-Use Probability\": round(prob_ai, 2)\n",
    "        }\n",
    "\n",
    "# ==================================\n",
    "# Audio Recorder (No changes needed)\n",
    "# ==================================\n",
    "def record_audio(stop_event, filename=\"output_audio.wav\"):\n",
    "    FORMAT, CHANNELS, RATE, CHUNK = pyaudio.paInt16, 1, 16000, 1024\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "    print(\"üé§ Audio recording started...\")\n",
    "    frames = []\n",
    "    while not stop_event.is_set():\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"üé§ Audio recording stopped. Saving file...\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "    print(f\"üé§ Audio saved to {filename}\")\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Speech Analyzer (No changes needed)\n",
    "# ==================================\n",
    "class SpeechAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.score = 0.0\n",
    "\n",
    "    def analyze_audio(self, filename=\"output_audio.wav\"):\n",
    "        try:\n",
    "            r = sr.Recognizer()\n",
    "            with sr.AudioFile(filename) as source:\n",
    "                audio = r.record(source)\n",
    "            text = r.recognize_google(audio)\n",
    "            print(\"üìù Transcript:\", text)\n",
    "            y, sr_rate = librosa.load(filename, sr=None)\n",
    "            pitch = librosa.yin(y, fmin=50, fmax=300)\n",
    "            pitch_std = np.std(pitch[~np.isnan(pitch)])\n",
    "            if pitch_std < 5: self.score += 0.4\n",
    "            if len(y) > 0 and len(text.split()) / (len(y) / sr_rate) < 1.5: self.score += 0.3\n",
    "            if not text: self.score += 0.3\n",
    "            return round(min(self.score, 1.0), 2), text\n",
    "        except Exception as e:\n",
    "            print(f\"Speech analysis failed: {e}. Returning default values.\")\n",
    "            return 0.5, \"\"\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Text Analyzer (No changes needed)\n",
    "# ==================================\n",
    "class TextAnalyzer:\n",
    "    def __init__(self, model_path=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.detector = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    def analyze(self, text):\n",
    "        if not text.strip():\n",
    "            return 0.5\n",
    "        result = self.detector(text)[0]\n",
    "        score = result['score'] if result['label'] == 'POSITIVE' else 1 - result['score']\n",
    "        return round(score, 2)\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Fusion Layer (No changes needed)\n",
    "# ==================================\n",
    "def fusion_layer(text_score, speech_score, video_score):\n",
    "    overall_score = 0.4 * text_score + 0.4 * speech_score + 0.2 * video_score\n",
    "    label = \"Uncertain\"\n",
    "    if overall_score < 0.45:\n",
    "        label = \"Likely Human\"\n",
    "    elif overall_score > 0.65:\n",
    "        label = \"Likely AI / Suspicious\"\n",
    "    return {\n",
    "        \"text_score\": text_score,\n",
    "        \"speech_score\": speech_score,\n",
    "        \"video_score\": video_score,\n",
    "        \"overall_score\": round(overall_score, 2),\n",
    "        \"verdict\": label\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Main Pipeline (UPDATED)\n",
    "# ==================================\n",
    "def main():\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False, device='cpu')\n",
    "    video_analyzer = VideoAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Calibration phase\n",
    "    print(\"Calibrating gaze... look at the CENTER for 3 seconds.\")\n",
    "    start_calib = time.time()\n",
    "    while time.time() - start_calib < 3:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if preds is not None:\n",
    "            video_analyzer.calibrate_center(frame, preds[0])\n",
    "        cv2.putText(frame, \"Look at CENTER for calibration\", (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        cv2.imshow(\"Calibration\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "    \n",
    "    cv2.destroyWindow(\"Calibration\")\n",
    "    if video_analyzer.calibrated:\n",
    "        print(\"‚úÖ Calibration complete.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Calibration failed. Using default center. The results may be less accurate.\")\n",
    "\n",
    "    # Start Audio Recording in Parallel\n",
    "    stop_audio_event = threading.Event()\n",
    "    audio_thread = threading.Thread(target=record_audio, args=(stop_audio_event, \"output_audio.wav\"))\n",
    "    audio_thread.start()\n",
    "\n",
    "    print(\"üé• Interview recording... Press 'q' in the video window to stop.\")\n",
    "\n",
    "    # Main monitoring loop\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        preds = fa.get_landmarks(frame.copy())\n",
    "        if preds is not None:\n",
    "            warning = video_analyzer.process_landmarks(frame, preds[0])\n",
    "            # Display real-time gaze warning\n",
    "            if warning:\n",
    "                cv2.putText(frame, warning, (30, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Interview Analysis\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            stop_audio_event.set()\n",
    "            break\n",
    "\n",
    "    # Cleanup and Analysis\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    audio_thread.join()\n",
    "    print(\"Video and Audio capture complete. Analyzing results...\")\n",
    "\n",
    "    # Get video report and extract score for fusion\n",
    "    video_report = video_analyzer.final_report()\n",
    "    video_score = video_report[\"AI-Use Probability\"]\n",
    "\n",
    "    # Run speech and text analysis\n",
    "    speech_analyzer = SpeechAnalyzer()\n",
    "    speech_score, transcript = speech_analyzer.analyze_audio(\"output_audio.wav\")\n",
    "    text_analyzer = TextAnalyzer()\n",
    "    text_score = text_analyzer.analyze(transcript)\n",
    "\n",
    "    # Fuse results\n",
    "    final_report = fusion_layer(text_score, speech_score, video_score)\n",
    "    \n",
    "    # Print detailed reports\n",
    "    print(\"\\n\" + \"=\"*25)\n",
    "    print(\"üìπ VIDEO ANALYSIS REPORT üìπ\")\n",
    "    print(\"=\"*25)\n",
    "    for k, v in video_report.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*25)\n",
    "    print(\"üî• FINAL FUSION REPORT üî•\")\n",
    "    print(\"=\"*25)\n",
    "    for k, v in final_report.items():\n",
    "        print(f\"{k.replace('_', ' ').title()}: {v}\")\n",
    "    print(\"=\"*25)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93fabd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating gaze... look at the CENTER for 3 seconds.\n",
      "‚úÖ Calibration complete.\n",
      "üé• Interview recording... Press 'q' in the video window to stop.\n",
      "üé§ Audio recording started...\n",
      "üé§ Audio recording stopped. Saving file...\n",
      "üé§ Audio saved to output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video and Audio capture complete. Analyzing results...\n",
      "Speech analysis failed: recognition connection failed: [Errno 11001] getaddrinfo failed. Returning default values.\n",
      "Loading model from: ./final_model\n",
      "ü§ñ Text analysis pipeline created successfully with your fine-tuned model.\n",
      "\n",
      "=========================\n",
      "üìπ VIDEO ANALYSIS REPORT üìπ\n",
      "=========================\n",
      "Total Time (s): 28.73\n",
      "Total Blinks: 7\n",
      "Suspected Head Movements: 0\n",
      "AI-Use Probability: 0.03\n",
      "\n",
      "=========================\n",
      "üî• FINAL FUSION REPORT üî•\n",
      "=========================\n",
      "Text Score: 0.5\n",
      "Speech Score: 0.5\n",
      "Video Score: 0.03\n",
      "Overall Score: 0.41\n",
      "Verdict: Likely Human\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_alignment\n",
    "import numpy as np\n",
    "import threading\n",
    "import wave\n",
    "import pyaudio\n",
    "import time\n",
    "import librosa\n",
    "import speech_recognition as sr\n",
    "from scipy.spatial import distance\n",
    "from collections import deque\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# ==================================\n",
    "# Video Analyzer (UPDATED based on your new code)\n",
    "# ==================================\n",
    "class VideoAnalyzer:\n",
    "    def __init__(self):\n",
    "        # General\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        # Blink Detection\n",
    "        self.total_blinks = 0\n",
    "        self.prev_ear = 0\n",
    "        self.blink_thresh = 0.2275\n",
    "\n",
    "        # Head Movement\n",
    "        self.prev_head_dir = 0\n",
    "        self.head_turn_thresh = 0.25\n",
    "        self.sus_head_movements = 0\n",
    "\n",
    "        # Gaze Tracking\n",
    "        self.gaze_history = deque(maxlen=10)\n",
    "        self.gaze_counts = {\"LEFT\": 0, \"RIGHT\": 0, \"CENTER\": 0}\n",
    "        self.gaze_warnings = 0\n",
    "\n",
    "        # Calibration\n",
    "        self.center_x = 0.5  # default normalized center\n",
    "        self.calibrated = False\n",
    "\n",
    "    def eye_aspect_ratio(self, eye):\n",
    "        \"\"\"Compute EAR for blink detection.\"\"\"\n",
    "        A = distance.euclidean(eye[1], eye[5])\n",
    "        B = distance.euclidean(eye[2], eye[4])\n",
    "        C = distance.euclidean(eye[0], eye[3])\n",
    "        return (A + B) / (2.0 * C)\n",
    "\n",
    "    def crop_eye(self, frame, eye_points):\n",
    "        \"\"\"Crop the eye region using bounding rect.\"\"\"\n",
    "        x_min = int(np.min(eye_points[:, 0]))\n",
    "        x_max = int(np.max(eye_points[:, 0]))\n",
    "        y_min = int(np.min(eye_points[:, 1]))\n",
    "        y_max = int(np.max(eye_points[:, 1]))\n",
    "        return frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    def detect_pupil(self, eye_img):\n",
    "        \"\"\"Detect pupil center using threshold + contours.\"\"\"\n",
    "        if eye_img.size == 0:\n",
    "            return None\n",
    "        gray = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.equalizeHist(gray)\n",
    "        _, thresh = cv2.threshold(gray, 40, 255, cv2.THRESH_BINARY_INV)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if len(contours) == 0:\n",
    "            return None\n",
    "\n",
    "        # Largest contour = pupil\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        return (x + w // 2, y + h // 2)\n",
    "\n",
    "    def get_head_direction(self, landmarks):\n",
    "        \"\"\"Rough head roll detection (nose-chin slope).\"\"\"\n",
    "        nose, chin = landmarks[30], landmarks[8]\n",
    "        return np.arctan2(chin[1] - nose[1], chin[0] - nose[0])\n",
    "\n",
    "    def calibrate_center(self, frame, landmarks):\n",
    "        \"\"\"Calibrate gaze by asking user to look at the center.\"\"\"\n",
    "        left_eye, right_eye = landmarks[36:42], landmarks[42:48]\n",
    "        x_vals = []\n",
    "        for eye_points in [left_eye, right_eye]:\n",
    "            eye_img = self.crop_eye(frame, eye_points)\n",
    "            pupil = self.detect_pupil(eye_img)\n",
    "            if pupil is not None and eye_img.shape[1] > 0:\n",
    "                cx, _ = pupil\n",
    "                norm_x = cx / (eye_img.shape[1] + 1e-6)\n",
    "                x_vals.append(norm_x)\n",
    "        if x_vals:\n",
    "            self.center_x = np.mean(x_vals)\n",
    "            self.calibrated = True\n",
    "\n",
    "    def process_landmarks(self, frame, landmarks):\n",
    "        \"\"\"Process landmarks to detect blinks, head movement, and eye gaze.\"\"\"\n",
    "        left_eye, right_eye = landmarks[36:42], landmarks[42:48]\n",
    "\n",
    "        # Blink detection\n",
    "        ear = (self.eye_aspect_ratio(left_eye) + self.eye_aspect_ratio(right_eye)) / 2.0\n",
    "        if ear < self.blink_thresh and self.prev_ear >= self.blink_thresh:\n",
    "            self.total_blinks += 1\n",
    "        self.prev_ear = ear\n",
    "\n",
    "        # Head movement\n",
    "        head_dir = self.get_head_direction(landmarks)\n",
    "        if self.prev_head_dir != 0 and abs(head_dir - self.prev_head_dir) > self.head_turn_thresh:\n",
    "            self.sus_head_movements += 1\n",
    "        self.prev_head_dir = head_dir\n",
    "\n",
    "        # Eye gaze detection\n",
    "        gaze_direction = \"CENTER\"\n",
    "        for eye_points in [left_eye, right_eye]:\n",
    "            eye_img = self.crop_eye(frame, eye_points)\n",
    "            pupil = self.detect_pupil(eye_img)\n",
    "            if pupil is not None and eye_img.shape[1] > 0:\n",
    "                cx, _ = pupil\n",
    "                norm_x = cx / (eye_img.shape[1] + 1e-6)\n",
    "                self.gaze_history.append(norm_x)\n",
    "\n",
    "        if len(self.gaze_history) > 0:\n",
    "            avg_x = np.mean(self.gaze_history)\n",
    "            if avg_x < self.center_x - 0.1:\n",
    "                gaze_direction = \"RIGHT\"  # User looks right, pupils move to left of eye frame\n",
    "            elif avg_x > self.center_x + 0.06:\n",
    "                gaze_direction = \"LEFT\" # User looks left, pupils move to right of eye frame\n",
    "            else:\n",
    "                gaze_direction = \"CENTER\"\n",
    "            self.gaze_counts[gaze_direction] += 1\n",
    "\n",
    "        # Return a warning for immediate feedback, only for off-center gaze\n",
    "        if gaze_direction in [\"LEFT\", \"RIGHT\"]:\n",
    "            self.gaze_warnings += 1\n",
    "            return f\"‚ö† GAZE WARNING: {gaze_direction}\"\n",
    "        return None\n",
    "\n",
    "    def final_report(self):\n",
    "        \"\"\"Generate a final dictionary with all video analysis stats.\"\"\"\n",
    "        elapsed = round(time.time() - self.start_time, 2)\n",
    "        # Simple AI-use probability scoring\n",
    "        prob_ai = (\n",
    "            (self.gaze_warnings * 0.4) +\n",
    "            (self.sus_head_movements * 0.3) +\n",
    "            (self.total_blinks * 0.3)\n",
    "        )\n",
    "        prob_ai = min(prob_ai / 100.0, 1.0)  # Normalize 0‚Äì1\n",
    "\n",
    "        return {\n",
    "            \"Total Time (s)\": elapsed,\n",
    "            \"Total Blinks\": self.total_blinks,\n",
    "            \"Suspected Head Movements\": self.sus_head_movements,\n",
    "            \"AI-Use Probability\": round(prob_ai, 2)\n",
    "        }\n",
    "\n",
    "# ==================================\n",
    "# Audio Recorder (No changes needed)\n",
    "# ==================================\n",
    "def record_audio(stop_event, filename=\"output_audio.wav\"):\n",
    "    FORMAT, CHANNELS, RATE, CHUNK = pyaudio.paInt16, 1, 16000, 1024\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "    print(\"üé§ Audio recording started...\")\n",
    "    frames = []\n",
    "    while not stop_event.is_set():\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"üé§ Audio recording stopped. Saving file...\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "    print(f\"üé§ Audio saved to {filename}\")\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Speech Analyzer (No changes needed)\n",
    "# ==================================\n",
    "class SpeechAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.score = 0.0\n",
    "\n",
    "    def analyze_audio(self, filename=\"output_audio.wav\"):\n",
    "        try:\n",
    "            r = sr.Recognizer()\n",
    "            with sr.AudioFile(filename) as source:\n",
    "                audio = r.record(source)\n",
    "            text = r.recognize_google(audio)\n",
    "            print(\"üìù Transcript:\", text)\n",
    "            y, sr_rate = librosa.load(filename, sr=None)\n",
    "            pitch = librosa.yin(y, fmin=50, fmax=300)\n",
    "            pitch_std = np.std(pitch[~np.isnan(pitch)])\n",
    "            if pitch_std < 5: self.score += 0.4\n",
    "            if len(y) > 0 and len(text.split()) / (len(y) / sr_rate) < 1.5: self.score += 0.3\n",
    "            if not text: self.score += 0.3\n",
    "            return round(min(self.score, 1.0), 2), text\n",
    "        except Exception as e:\n",
    "            print(f\"Speech analysis failed: {e}. Returning default values.\")\n",
    "            return 0.5, \"\"\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Text Analyzer (UPDATED with your new code)\n",
    "# ==================================\n",
    "class TextAnalyzer:\n",
    "    def __init__(self, model_path=\"./final_model\"):\n",
    "        \"\"\"Loads the fine-tuned model and creates a text-classification pipeline.\"\"\"\n",
    "        self.detector = None\n",
    "        try:\n",
    "            print(f\"Loading model from: {model_path}\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            self.detector = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "            print(\"ü§ñ Text analysis pipeline created successfully with your fine-tuned model.\")\n",
    "        except OSError:\n",
    "            print(f\"ERROR: Model not found at '{model_path}'.\")\n",
    "            print(\"Text analysis will be skipped. Please check the model path.\")\n",
    "\n",
    "\n",
    "    def analyze(self, text):\n",
    "        \"\"\"Analyzes the text using the fine-tuned model.\"\"\"\n",
    "        # If the model failed to load or there's no text, return a neutral score.\n",
    "        if self.detector is None or not text.strip():\n",
    "            return 0.5\n",
    "        \n",
    "        # Use the pipeline to get the prediction\n",
    "        result = self.detector(text)\n",
    "        prediction = result[0]\n",
    "        label = prediction['label']\n",
    "        score = prediction['score']\n",
    "\n",
    "        print(f\"Text Analysis Prediction: Label={label}, Confidence={score:.2%}\")\n",
    "\n",
    "        # Convert the label and score to a single AI-probability score\n",
    "        # This assumes your model's labels are like 'AI-Generated' and 'Human-Written'\n",
    "        # Adjust the label name if your model uses different ones (e.g., LABEL_1, LABEL_0)\n",
    "        if \"AI\" in label.upper() or \"MACHINE\" in label.upper():\n",
    "            return round(score, 2)\n",
    "        else: # Assumes the other label is Human-Written\n",
    "            return round(1 - score, 2)\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Fusion Layer (No changes needed)\n",
    "# ==================================\n",
    "def fusion_layer(text_score, speech_score, video_score):\n",
    "    overall_score = 0.4 * text_score + 0.4 * speech_score + 0.2 * video_score\n",
    "    label = \"Uncertain\"\n",
    "    if overall_score < 0.45:\n",
    "        label = \"Likely Human\"\n",
    "    elif overall_score > 0.65:\n",
    "        label = \"Likely AI / Suspicious\"\n",
    "    return {\n",
    "        \"text_score\": text_score,\n",
    "        \"speech_score\": speech_score,\n",
    "        \"video_score\": video_score,\n",
    "        \"overall_score\": round(overall_score, 2),\n",
    "        \"verdict\": label\n",
    "    }\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# Main Pipeline (UPDATED)\n",
    "# ==================================\n",
    "def main():\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False, device='cpu')\n",
    "    video_analyzer = VideoAnalyzer()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Calibration phase\n",
    "    print(\"Calibrating gaze... look at the CENTER for 3 seconds.\")\n",
    "    start_calib = time.time()\n",
    "    while time.time() - start_calib < 3:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        preds = fa.get_landmarks(frame)\n",
    "        if preds is not None:\n",
    "            video_analyzer.calibrate_center(frame, preds[0])\n",
    "        cv2.putText(frame, \"Look at CENTER for calibration\", (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        cv2.imshow(\"Calibration\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "    \n",
    "    cv2.destroyWindow(\"Calibration\")\n",
    "    if video_analyzer.calibrated:\n",
    "        print(\"‚úÖ Calibration complete.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Calibration failed. Using default center. The results may be less accurate.\")\n",
    "\n",
    "    # Start Audio Recording in Parallel\n",
    "    stop_audio_event = threading.Event()\n",
    "    audio_thread = threading.Thread(target=record_audio, args=(stop_audio_event, \"output_audio.wav\"))\n",
    "    audio_thread.start()\n",
    "\n",
    "    print(\"üé• Interview recording... Press 'q' in the video window to stop.\")\n",
    "\n",
    "    # Main monitoring loop\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        preds = fa.get_landmarks(frame.copy())\n",
    "        if preds is not None:\n",
    "            warning = video_analyzer.process_landmarks(frame, preds[0])\n",
    "            # Display real-time gaze warning\n",
    "            if warning:\n",
    "                cv2.putText(frame, warning, (30, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Interview Analysis\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            stop_audio_event.set()\n",
    "            break\n",
    "\n",
    "    # Cleanup and Analysis\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    audio_thread.join()\n",
    "    print(\"Video and Audio capture complete. Analyzing results...\")\n",
    "\n",
    "    # Get video report and extract score for fusion\n",
    "    video_report = video_analyzer.final_report()\n",
    "    video_score = video_report[\"AI-Use Probability\"]\n",
    "\n",
    "    # Run speech and text analysis\n",
    "    speech_analyzer = SpeechAnalyzer()\n",
    "    speech_score, transcript = speech_analyzer.analyze_audio(\"output_audio.wav\")\n",
    "    text_analyzer = TextAnalyzer() # Will load your fine-tuned model\n",
    "    text_score = text_analyzer.analyze(transcript)\n",
    "\n",
    "    # Fuse results\n",
    "    final_report = fusion_layer(text_score, speech_score, video_score)\n",
    "    \n",
    "    # Print detailed reports\n",
    "    print(\"\\n\" + \"=\"*25)\n",
    "    print(\"üìπ VIDEO ANALYSIS REPORT üìπ\")\n",
    "    print(\"=\"*25)\n",
    "    for k, v in video_report.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*25)\n",
    "    print(\"üî• FINAL FUSION REPORT üî•\")\n",
    "    print(\"=\"*25)\n",
    "    for k, v in final_report.items():\n",
    "        print(f\"{k.replace('_', ' ').title()}: {v}\")\n",
    "    print(\"=\"*25)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
